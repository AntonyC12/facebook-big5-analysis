import time
import sys
from src.scraper import FacebookScraper
from src.personality import BigFiveAnalyzer
from src.utils import save_json, format_duration
from config import TARGET_PROFILE_URL, MAX_POSTS, HEADLESS_BROWSER

def main():
    print("ğŸš€ INICIANDO ANÃLISIS DE PERSONALIDAD FACEBOOK")
    print("="*60)
    print(f"ğŸ“ Perfil objetivo: {TARGET_PROFILE_URL}")
    print(f"ğŸ“Š MÃ¡ximo de posts: {MAX_POSTS}")
    print(f"ğŸ‘ï¸  Modo headless: {HEADLESS_BROWSER}")
    print("="*60)
    
    start_time = time.time()
    
    try:
        # 1. SCRAPING
        print("\nğŸ” Fase 1: Scraping de datos...")
        with FacebookScraper(headless=HEADLESS_BROWSER) as scraper:
            # Asegurar login
            scraper.ensure_login()
            
            # Navegar al perfil
            scraper.navigate_to_profile(TARGET_PROFILE_URL)
            
            # Extraer datos
            print("   ğŸ“‹ Extrayendo informaciÃ³n bÃ¡sica...")
            basic_info = scraper.extract_basic_info()
            print(f"      âœ“ Nombre: {basic_info.get('name', 'No encontrado')}")
            
            print(f"   ğŸ“„ Extrayendo hasta {MAX_POSTS} publicaciones...")
            posts = scraper.extract_posts(max_posts=MAX_POSTS)
            print(f"      âœ“ {len(posts)} publicaciones obtenidas")
            
            print("   ğŸ‘¥ Extrayendo amigos...")
            friends_count = scraper.extract_friends_count()
            print(f"      âœ“ {friends_count} amigos detectados")
            
            print("   ğŸ‘¥ Extrayendo grupos...")
            groups = scraper.extract_groups()
            print(f"      âœ“ {len(groups)} grupos encontrados")
            
            # Compilar datos
            sample_data = {
                "basic_info": basic_info,
                "posts": posts,
                "friends_count": friends_count,
                "groups": groups,
                "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        
        # Guardar datos crudos
        raw_file = save_json(sample_data, "facebook_data")
        print(f"âœ… Scraping completado - Datos en: {raw_file}")
        
        # 2. ANÃLISIS
        print("\nğŸ§  Fase 2: AnÃ¡lisis de personalidad...")
        analyzer = BigFiveAnalyzer()
        
        # Calcular puntuaciones
        scores = analyzer.calculate_big_five_scores(sample_data)
        
        # Generar reporte
        report = analyzer.generate_personality_report(scores)
        
        print("\n" + "="*60)
        print("ğŸ“Š RESULTADOS BIG FIVE:")
        print("="*60)
        print(report)
        print("="*60)
        
        # Guardar resultados
        analyzer.save_results("big5_analysis")
        
        # 3. ESTADÃSTICAS
        duration = time.time() - start_time
        print(f"\nâ±ï¸  DuraciÃ³n total: {format_duration(duration)}")
        print(f"ğŸ“Š Publicaciones analizadas: {analyzer.results['metadata']['posts_analyzed']}")
        print(f"ğŸ”¤ Palabras analizadas: {analyzer.results['metadata']['words_analyzed']:,}")
        print(f"ğŸ’¾ Resultados guardados en: data/results/")
        print("\nğŸ‰ AnÃ¡lisis completado exitosamente!")
        
        # Mostrar ubicaciÃ³n archivos
        print("\nğŸ“ ARCHIVOS GENERADOS:")
        print(f"   â€¢ Datos crudos: data/raw_json/facebook_data_*.json")
        print(f"   â€¢ Resultados JSON: data/results/big5_analysis_*.json")
        print(f"   â€¢ Reporte texto: data/results/big5_analysis_*.txt")
        
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Proceso cancelado por el usuario")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Error durante la ejecuciÃ³n: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()